{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polygenic Risk Scores Combinations\n",
    "The Polygenic Risk Score Combinations Project Version 1.0\n",
    "\n",
    "Follow this Jupyter Notebook step-by-step to replicate our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup config.py File\n",
    "1. Set parent directories\n",
    "2. Name file names of your input files\n",
    "    - Input files should be PRSKB output, adnimerge and dxsum\n",
    "3. Define columns of interest for your DataFrames\n",
    "    - All TSV columns listed are mandatory\n",
    "    - ADNI columns 'RID' and 'PTID' are mandatory\n",
    "    - All DX columns are mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import tsv_path, tsv_cols, adni_path, adni_cols, dx_path, dx_cols\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import gmean, hmean, ranksums, chisquare\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize DataFrames and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_df = pd.read_csv(tsv_path, header=0, usecols=tsv_cols,sep='\\t')\n",
    "adni_df = pd.read_csv(adni_path, header=0,usecols=adni_cols)\n",
    "dx_df = pd.read_csv(dx_path,header=0,usecols=dx_cols)\n",
    "demographics = ['PTID','AGE','PTGENDER','PTRACCAT','APOE4','DIAGNOSIS','EXAMDATE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Filtered Dataframe for Reported Trait\n",
    "- tsv_df will be the input dataframe\n",
    "- traits arg example: 'Alzheimer|Dementia'\n",
    "- Separate multiple traits by a pipe operator\n",
    "- Example usage filtered_df = filter_df(tsv_df,'Liver|Hepatic|Blood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df,traits):\n",
    "    df = df[df['Reported Trait'].str.contains(f'{traits}', case=False)].reset_index(drop=True)\n",
    "    return df\n",
    "filtered_df = filter_df(tsv_df,'Alzheimer|Dementia')\n",
    "display(filtered_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find the Earliest or Latest Diagnosis for a Patient in ADNI\n",
    "- df is dx_df\n",
    "- Options are 'Earliest' or 'Latest' EXAMDATE. For example, 'Earliest' would be 02/25/2006 andd 'Latest' would be 06/17/2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diagnosis(df,options):\n",
    "    # Define options\n",
    "    if options == 'Earliest':\n",
    "        keep = 'first'\n",
    "        fill = 'bfill'\n",
    "    elif options == 'Latest':\n",
    "        keep = 'last'\n",
    "        fill = 'ffill'\n",
    "    else: \n",
    "        raise ValueError(\"Enter 'Earliest' or 'Latest'\")\n",
    "\n",
    "    # Fill empty values depending on option selected\n",
    "    df['EXAMDATE'] = df['EXAMDATE'].fillna(method=fill)\n",
    "    df['DIAGNOSIS'] = df['DIAGNOSIS'].fillna(method=fill)\n",
    "\n",
    "    # Keep only the earliest or latest diagnosis depending on the option selected\n",
    "    df = df.drop_duplicates(subset='RID', keep=keep).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "diagnoses = find_diagnosis(dx_df,'Latest') # Example usage\n",
    "display(diagnoses.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Merge Three DataFrames to Clean the Data\n",
    "- demographics must be a list datatype.\n",
    "- df1 = adni_df\n",
    "- df2 = diagnoses. This must be generated from find_diagnosis function\n",
    "- df3 = tsv_df\n",
    "- You can refine the indices using the demographics argument. This selects demographics from ADNI, but you can only select those loaded in the original config.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_df(df1,df2,df3,demographics):\n",
    "    # demographics must be a list variable of demographics from adni_df\n",
    "    # df1 = adni_df\n",
    "    # df2 = diagnoses. This must be generated from find_diagnosis function\n",
    "    # df3 = filtered_df\n",
    "    # Drops duplicates in adni_df so that we only have a person's demographics once in the merged dataframe\n",
    "    df1 = df1.drop_duplicates(subset='RID', keep='first').reset_index(drop=True)\n",
    "    # Create Merged DataFrame on RID\n",
    "    adni_diagnoses = pd.merge(df1,df2, left_on='RID',right_on='RID',how='inner').reset_index(drop=True)\n",
    "    # Make sure the case is the same for each DataFrame, otherwise you will delete some of the samples\n",
    "    adni_diagnoses['PTID'] = adni_diagnoses['PTID'].str.upper()\n",
    "    df3['Sample'] = df3['Sample'].str.upper()\n",
    "    # Create Merged DataFrame on Sample\n",
    "    diagnoses_tsv = pd.merge(adni_diagnoses,df3, left_on='PTID', right_on='Sample', how='inner').reset_index(drop=True)\n",
    "    #Create Pivot Table to Map Sample to their Percentile Scores\n",
    "    merged_df = diagnoses_tsv.pivot_table(index=demographics, columns='Study ID', values='Percentile',aggfunc='first')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "merged = create_merged_df(adni_df,diagnoses,filtered_df,demographics=demographics)\n",
    "display(merged.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Convert Range PRS to Mean of Lower and Upper Bounds\n",
    "- Takes merged as df argument\n",
    "- Converts UK Biobank Percentile Ranges, like 50-72 to numeric: 61.0\n",
    "- This is a key step for future analysis steps, such as the means or Chi-squared tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_to_numeric(df):\n",
    "    # Converts UK Biobank Percentile Ranges, like 50-72 to numeric: 61.0 \n",
    "    # Input df should be the merged df, but could also be tsv_df\n",
    "    def convert_value(x):\n",
    "        if pd.isna(x):\n",
    "            return None\n",
    "        elif '-' in x:\n",
    "            lower, upper = map(float, x.split('-'))\n",
    "            return (lower + upper) / 2\n",
    "        else:\n",
    "            return float(x)\n",
    "\n",
    "    # Apply the conversion to all columns in the DataFrame\n",
    "    return df.applymap(convert_value)\n",
    "\n",
    "numeric_prs = range_to_numeric(merged)\n",
    "display(numeric_prs.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Drop Genome-Wide Association Studies\n",
    "- Drop Genome-Wide Association Studies (GWAS) according to your preferred method\n",
    "- In our study, we set unique to 50, meaning we got rid of GWAS that computed 50 or less unique PRS across the samples\n",
    "- This resulted in 19 GWAS for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_gwas(df,methods,threshold=0,gwas=None,unique=0):\n",
    "    if gwas == None:\n",
    "        gwas = [] # Initialize gwas as an empty list\n",
    "\n",
    "    # Drop GWAS with preferred method\n",
    "    if methods == 'dropna':\n",
    "        # drop GWAS with NaN PRS values for any individuals\n",
    "        df = df.dropna(axis='columns',how='any')\n",
    "    elif methods == 'dropnaX':\n",
    "        # drop GWAS that have X number of NaN PRS values\n",
    "        df = df.dropna(axis='columns',thresh=threshold)\n",
    "    elif methods == 'repeated':\n",
    "        # Calculate the proportion of unique values in each column\n",
    "        unique_proportion = df.apply(lambda col: col.nunique() / col.count(), axis='rows')\n",
    "        # Filter columns where the proportion of unique values in a column is less than or equal to \n",
    "        # We would expect unique to equal 100 since PRS are percentiles. This means 100/808 is the expected unique proportion.\n",
    "        # Set the appropriate cutoff for your needs. We used unique = 50\n",
    "        del_cols = unique_proportion[unique_proportion <= (unique/len(df))].index\n",
    "        # Drop columns with poor uniqueness\n",
    "        df = df.drop(del_cols, axis='columns')\n",
    "    elif methods == 'select':\n",
    "        # removes custom GWAS that you select\n",
    "        # gwas must be a list of GWAIDs. Ex: ['GCST000001','GCST000006']\n",
    "        df = df.drop(gwas,axis='columns')\n",
    "    else:\n",
    "        raise ValueError(\"Enter specified method\")\n",
    "\n",
    "    return df\n",
    "cleaned_df = drop_gwas(numeric_prs,'dropna')\n",
    "unique_gwas = drop_gwas(cleaned_df,'repeated',unique=50)\n",
    "display(unique_gwas.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Calculate Means \n",
    "- Calculate means of UK Biobank Percentile Polygenic Risk Scores (PRS) for each individual.\n",
    "- df must be numeric_prs. The mean functions only take numeric datatypes.\n",
    "- method can be mean for arithmetic mean, gmean for geometric mean, or hmean for harmonic mean. See numpy and scipy stats docs for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def means_calculations(df):\n",
    "    # Remove all columns with NaN valuea and replace all UK Biobank 0th percentile scores with 1 so that .gmean() and .hmean() functions work\n",
    "    df.dropna(axis='columns',how='any',inplace=True)\n",
    "    df.replace(0.0, 1, inplace=True)\n",
    "    # # Calculate Arithmetic, Geometric, and Harmonic Means Using .apply function. Add new columns with results\n",
    "    df['Arithmetic Mean'] = df.apply(np.mean, axis=1)\n",
    "    df['Geometric Mean'] = df.apply(gmean, axis=1)\n",
    "    df['Harmonic Mean'] = df.apply(hmean, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "means_df = means_calculations(unique_gwas)\n",
    "display(means_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Simple Diagnosis for Cases and Controls\n",
    "- Merges definitions for cases into one name.\n",
    "- In our study a case is cognitive impairment, \"CI\" and a control is cognitive normal, \"CN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpledx(df):\n",
    "    # Turning all indices into columns\n",
    "    df.reset_index(inplace=True)\n",
    "    # Changes DIAGNOSIS of Dementia or MCI to CI, which means Cognitive Impairment\n",
    "    df['DIAGNOSIS'] = df['DIAGNOSIS'].replace([\"Dementia\", \"MCI\"], \"CI\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "final_df = simpledx(means_df)\n",
    "display(final_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Mann-Whitney U Test\n",
    "- Non-parametric T test used to determine if polygenic risk scores can discern between cases and controls.\n",
    "- Arguments: simple_diagnosis DataFrame and GWAS to run the test on. Default is all GWAS and means.\n",
    "- Output: mannwhitney DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mannwhitneyu(df):\n",
    "  ## Performs Mann-Whitney U test on each column between 'CN' and 'CI' diagnosis groups\n",
    "  # Args: The pandas dataframe simple diagnosis\n",
    "  # Returns: A DataFrame with columns 'Columns','Statistic',and 'P-value' for each test\n",
    "  results_df = pd.DataFrame(columns=['Columns', 'Statistic', 'P-value'])\n",
    "\n",
    "  default = [col for col in df.columns if col not in demographics]\n",
    "  for column in default:\n",
    "    try:\n",
    "      group1 = df[df['DIAGNOSIS'] == 'CN'][column].dropna()\n",
    "      group2 = df[(df['DIAGNOSIS'] == 'CI')][column].dropna()\n",
    "      statistic, p_value = ranksums(group1, group2)\n",
    "    except ValueError:\n",
    "      statistic, p_value = np.nan, np.nan  # Use np.nan for missing values\n",
    "\n",
    "    results_df = pd.concat([results_df,pd.DataFrame({'Columns': [column], 'Statistic': [statistic], 'P-value': [p_value]})], ignore_index=True)\n",
    "\n",
    "  return results_df\n",
    "\n",
    "mannwhitney = mannwhitneyu(final_df)\n",
    "display(mannwhitney)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Chi-Squared Test\n",
    "- Test of independence used to determine if polygenic risk scores can discern between cases and controls in the top quintile of risk.\n",
    "- Arguments: simple_diagnosis DataFrame, quantile cutoff, and GWAS to run the test on. Default is all GWAS and means. Quantile in our case was 0.8, meaning we looked at the top 0.2 PRS.\n",
    "- Output: chisq DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared(df,quantile):\n",
    "    #quantile is the lower cutoff for risk. If you pick 0.8, you will calculate Chi-squared for the top 20% of PRS\n",
    "    results_df = pd.DataFrame(columns=['Columns','Statistic','P-value'])\n",
    "\n",
    "    default = [col for col in df.columns if col not in demographics]\n",
    "    for column in default:\n",
    "        df_sorted = df.sort_values(by=column,ascending = False)\n",
    "    \n",
    "        cn_count = sum(df[\"DIAGNOSIS\"] == 'CN')\n",
    "        ci_count = sum(df[\"DIAGNOSIS\"] == 'CI')\n",
    "\n",
    "        threshold = int((1-quantile) * len(df_sorted))\n",
    "        observed_cn = sum(df_sorted[\"DIAGNOSIS\"].iloc[:threshold] == 'CN')\n",
    "        observed_ci = sum(df_sorted[\"DIAGNOSIS\"].iloc[:threshold] == 'CI')\n",
    "        observed = np.array([observed_cn, observed_ci])\n",
    "\n",
    "        expected = np.array([(cn_count/len(df_sorted)),(ci_count/len(df_sorted))]) * np.sum(observed)\n",
    "        chisq_result, p_value = chisquare(observed, expected)\n",
    "        \n",
    "        temp_df = pd.DataFrame({'Columns': [column], 'Statistic': [chisq_result], 'P-value': [p_value]})\n",
    "\n",
    "        # Concatenate results_df with temp_df\n",
    "        results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "chisq = chi_squared(final_df,0.8)\n",
    "display(chisq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Make Plots\n",
    "- Make a Mann Whitney U Test Plot or a Chi-Squared test of independence plot\n",
    "- Arguments: mannwhitney DataFrame for Mann-Whitney U Test plot, or chisq DataFrame for Chi-Squared plot. Second argument, options is the name of the plot.\n",
    "- Name of the plot is either \"Chi-Squared Test\" or \"Mann-Whitney U Test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(df,options):\n",
    "    #Sort P-Values so that bars increase in height with significance\n",
    "    df_sorted = df.sort_values('P-value', ascending=False)\n",
    "\n",
    "    # Extract sorted lists for plotting\n",
    "    gwas_studies = df_sorted['Columns'].tolist()\n",
    "    p_values = df_sorted['P-value'].tolist()\n",
    "\n",
    "    # Calculate the negative log of the p-values\n",
    "    neg_log_p_values = [-np.log10(p) if p > 0 else np.nan for p in p_values]  # Using nan for p-value of 0 (log(0) is undefined)\n",
    "\n",
    "    # Create the bar graph\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    colors = ['skyblue' for study in gwas_studies]\n",
    "    bars = plt.bar(gwas_studies, neg_log_p_values, color=colors)\n",
    "\n",
    "    # Add a line for the typical significance threshold (e.g., p < 0.05)\n",
    "    significance_threshold = -np.log10(0.05)\n",
    "    plt.axhline(y=significance_threshold, color='red', linestyle='--', label='Significance threshold (p=0.05)')\n",
    "\n",
    "    # Annotate the bars with the actual p-values\n",
    "    for bar, p_value in zip(bars, p_values):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'p={p_value:.3g}', \n",
    "                ha='center', va='bottom', fontsize=6,rotation='horizontal')\n",
    "\n",
    "    # Setting the labels and title\n",
    "    plt.xlabel('GWAS Study', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('-log10(p-value)', fontsize=12, fontweight='bold')\n",
    "    plt.title(f'{options} Results', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=90, fontsize=10, fontweight='bold')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "chisq_plot = make_plots(chisq,\"Chi-Squared Test\")\n",
    "mannwhitney_plot = make_plots(mannwhitney, \"Mann-Whitney U Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Save Output\n",
    "- Save final DataFrame 'final_df' as a CSV, TSV, or Excel (XLSX) file for viewing or further analysis\n",
    "- Arguments: DataFrame, Filetype, and Path on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(df, option, path):\n",
    "    # Options are 'CSV','TSV', or 'XLSX'\n",
    "    if option == 'CSV':\n",
    "        df.to_csv(path,index=True)\n",
    "        print(\"CSV File Saved\")\n",
    "    elif option == 'TSV':\n",
    "        df.to_csv(path,index=True,sep='\\t')\n",
    "        print(\"TSV File Saved\")\n",
    "    elif option == 'XLSX':\n",
    "        df.to_excel(path,index=True)\n",
    "        print(\"XLSX (Excel) File Saved\")\n",
    "    else: \n",
    "        print(\"Pick filetype option: 'CSV','TSV', or 'XLSX' \")\n",
    "\n",
    "# final_csv = save_output(final_df,'CSV',path) # Example usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
